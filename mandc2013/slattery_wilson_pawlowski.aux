\relax 
\citation{Tautges_2009_2}
\citation{Edwards_2006}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em. INTRODUCTION}{1}}
\newlabel{sec:intro}{{1}{1}}
\citation{u.s._department_of_energy_casl_2011}
\citation{Plimpton_2004}
\citation{Stewart_2004}
\citation{Chand_2008}
\citation{Plimpton_2004}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em. GEOMETRIC RENDEZVOUS}{2}}
\newlabel{sec:geometric_rendezvous}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em. Rendezvous Decomposition}{2}}
\newlabel{subsec:rendezvous_algorithm}{{2.1}{2}}
\citation{Berger_1987}
\citation{Bentley_1975}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Rendezvous decomposition example. A triangle mesh (left) and a quadrilateral mesh (center) are partitioned into 4 parallel domains as indicated by color. The rendezvous decomposition (right) is generated as a geometric-based repartitioning of the source mesh that permits load balanced geometric operations.}}{3}}
\newlabel{fig:rendezvous_example}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em. Parallel Topology Maps}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Shared domain example. $\Omega (S)$ (yellow) is the source geometry, $\Omega (T)$ (blue) is the target geometry, and $\Omega (R)$ (red) is the shared domain.}}{4}}
\newlabel{fig:shared_domain}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\hskip -1em. Extension to General Geometries}{4}}
\newlabel{subsec:general_geometry}{{2.3}{4}}
\citation{Plimpton_2004}
\citation{u.s._department_of_energy_casl_2011}
\citation{drekar_cfd}
\citation{denovo_2010}
\citation{shadid_2006}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em. DATA TRANSFER EXAMPLE USE CASE}{5}}
\newlabel{sec:examples}{{3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em. Data Transfer Verification}{5}}
\newlabel{subsec:cht}{{3.1}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Parallel decomposition for coupled NE/TH verification. The colors indicate ownership of the element by a core. In this example, a 10 core job is run with the thermal hydraulics and neutronics applications each owning 5 cores on separate processor spaces.}}{6}}
\newlabel{fig:parallel_decomposition}{{3}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces (Left) Verification simulation domain space consisting of a top-down view of a single fuel pin surrounded by a fluid region. The blue region represents the fluid area and the red region represents the solid fuel pin. (Right) Multiphysics domain space.}}{6}}
\newlabel{fig:domain}{{4}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em. PARALLEL SCALING STUDY}{6}}
\newlabel{sec:scaling_study}{{4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em. Weak Scaling}{7}}
\newlabel{subsec:weak_scaling}{{4.1}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Weak scaling study results. The solid black curve reports the wall time to generate the mapping vs. number of processors while the solid red curve reports the wall time to transfer the data vs. number of processors. The dashed lines give perfect weak scaling the map generation (black) and the data transfer (red).}}{7}}
\newlabel{fig:weak_scaling}{{5}{7}}
\citation{Plimpton_2004}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces \sl  Weak scaling study data with the local problem size fixed to 1.0E4 elements/points. All times reported in seconds. Minimum, maximum, and average timing values are global and computed using the results from all processes.}}{8}}
\newlabel{tab:weak_scaling}{{I}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces \sl  Weak scaling efficiencies. The 16 process case was used as the reference case.}}{8}}
\newlabel{tab:weak_efficiency}{{II}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em. Strong Scaling}{8}}
\newlabel{subsec:strong_scaling}{{4.2}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Strong scaling study results. The solid black curve reports the wall time to generate the mapping vs. number of processors while the solid red curve reports the wall time to transfer the data vs. number of processors. The dashed lines give perfect strong scaling the map generation (black) and the data transfer (red).}}{9}}
\newlabel{fig:strong_scaling}{{6}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces \sl  Strong scaling study data. All times reported in seconds. Minimum, maximum, and average timing values are global and computed using the results from all processes.}}{9}}
\newlabel{tab:strong_scaling}{{III}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces \sl  Strong scaling efficiencies. The 256 process case was used as the reference case.}}{9}}
\newlabel{tab:strong_efficiency}{{IV}{9}}
\bibstyle{ieeetr}
\bibdata{references}
\bibcite{Tautges_2009_2}{1}
\bibcite{Edwards_2006}{2}
\bibcite{u.s._department_of_energy_casl_2011}{3}
\bibcite{Plimpton_2004}{4}
\bibcite{Stewart_2004}{5}
\bibcite{Chand_2008}{6}
\bibcite{Berger_1987}{7}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em. CONCLUSIONS}{10}}
\bibcite{Bentley_1975}{8}
\bibcite{drekar_cfd}{9}
\bibcite{denovo_2010}{10}
\bibcite{shadid_2006}{11}
\global \c@l@stpage 11\relax \global \advance \c@l@stpage 0\relax 
